{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JAXMg","text":"<p>JAXMg provides a C++ interface between JAX and cuSolverMg, NVIDIA\u2019s multi-GPU linear solver.  We provide a jittable API for the following routines.</p> <ul> <li>cusolverMgPotrs: Solves the system of linear equations: \\(Ax=b\\) where \\(A\\) is an \\(N\\times N\\) symmetric (Hermitian) positive-definite matrix via a Cholesky decomposition </li> <li>cusolverMgPotrs: Computes the inverse of an \\(N\\times N\\) symmetric (Hermitian) positive-definite matrix via a Cholesky decomposition.</li> <li>cusolverMgPotrs: Computes eigenvalues and eigenvectors of an \\(N\\times N\\) symmetric (Hermitian) matrix.</li> </ul> <p>For more details, see the API.</p> <p>The provided binaries are compiled with:</p> Component Version GCC 11.5.0 CUDA 12.8.0 cuDNN 9.2.0.82-12 <p>Warning</p> <p>We require JAX&gt;=0.6.0, since it ships with CUDA 12.x binaries, which this package relies on. No local version of CUDA is required.</p>"},{"location":"install/","title":"Installation","text":"<p>Clone the repository and install with:</p> <pre><code>pip install \".[cuda]\"\n</code></pre> <p>This will install a GPU compatible version of JAX. </p> <p>To verify the installation (requires at least one GPU) run</p> <p><pre><code>pytest \n</code></pre> There are two types of tests:</p> <ol> <li>SPMD tests: Single Process Multiple GPU tests.</li> <li>MPMD: Multiple Processes Multiple GPU tests.</li> </ol>"},{"location":"api/","title":"API Reference","text":"<p>This page highlights the three primary public functions from the <code>jaxmg</code> package. Supported datatypes are <code>jax.numpy.float32</code>, <code>jax.numpy.float64</code>, <code>jax.numpy.complex64</code> and <code>jax.numpy.complex128</code>.</p> <p>All multi-GPU solvers in called by JAXMg expect a 1D block-cyclic column layout at the device level \u2014 a tiled, round-robin distribution of columns across devices driven by the tile width <code>T_A</code> used by the native kernels.  The conversion between the natural row-sharded JAX input and the 1D block-cyclic layout is performed internally in the C++/CUDA layer. Users can pass normal row-sharded matrices to the high-level functions; the library handles the remapping and padding required by the native kernels so you don't have to manage the cyclic layout yourself.</p> <p>Warning</p> <p>The user must supply a tile width <code>T_A</code> to the solvers. Choose <code>T_A</code> carefully: very small values (e.g. &lt; 128) can make the native kernels much slower. Furthermore, if the shard size of the matrix is not a multiple of <code>T_A</code> we must add per-device padding to fit the last tile \u2014 that padding requires copying data and increases memory use and runtime. In short: prefer a reasonably large <code>T_A</code> (&gt;=128) and, where possible, pick <code>T_A</code> so that your shard size is an exact multiple to avoid copying and unnecessary slowdown.</p>"},{"location":"api/#potrs","title":"potrs","text":"<p>Multi-GPU Cholesky linear solver for symmetric (Hermitian) positive-definite matrices.</p> \\[ A x = B, \\quad A = L L^{\\top} \\;\\text{(real)} \\quad \\text{or} \\quad A = L L^{\\dagger}\\;\\text{(complex)} \\] <p>Solve for \\(x\\) using the Cholesky factors.</p> <p>Full potrs module \u2192</p>"},{"location":"api/#potri","title":"potri","text":"<p>Multi-GPU matrix inversion helper for symmetric (Hermitian) positive-definite matrices.</p> \\[ A^{-1} = (L L^{\\top})^{-1} = L^{-\\top} L^{-1} \\quad\\text{(real)}\\quad\\text{or}\\quad A^{-1} = L^{-\\dagger} L^{-1} \\;\\text{(complex)} \\] <p>Compute the inverse (or the upper-triangular part) of using Cholesky the Cholesky factors.</p> <p>Full potri module \u2192</p>"},{"location":"api/#syevd","title":"syevd","text":"<p>Multi-GPU eigensolver for symmetric (Hermitian) matrices.</p> \\[ A v = \\lambda v \\quad\\Rightarrow\\quad A = V \\Lambda V^{\\top} \\;\\text{(real)}\\quad\\text{or}\\quad A = V \\Lambda V^{\\dagger} \\;\\text{(complex)} \\] <p>Compute eigenvalues \\(\\Lambda\\) and (optionally) eigenvectors \\(V\\) of a symmetric (Hermitian) matrix.</p> <p>Full syevd module \u2192</p>"},{"location":"api/cyclic_1d/","title":"jaxmg.cyclic_1d","text":""},{"location":"api/cyclic_1d/#jaxmg.cyclic_1d","title":"<code>jaxmg.cyclic_1d(a, T_A, mesh, in_specs, pad=True)</code> <code></code>","text":"<p>Prepare and run the 1D block-cyclic remapping FFI kernel for row-sharded arrays.</p> <p>Converts a row-sharded 2D array into the 1D block-cyclic layout expected by the native multi-GPU solvers, handling per-device padding, invocation of the FFI kernel under <code>jax.jit</code> and <code>jax.shard_map</code>, and removal of any temporary padding.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>A 2D JAX array of shape <code>(N_rows, N)</code>. Must be sharded across the mesh along the first (row) axis using a single <code>PartitionSpec</code> (i.e. <code>P(&lt;axis_name&gt;, None)</code>).</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough. In principle, the larger <code>T_A</code> the faster the solver runs.</p> required <code>mesh</code> <code>Mesh</code> <p>JAX device mesh used for <code>jax.shard_map</code>.</p> required <code>in_specs</code> <code>PartitionSpec or tuple / list[PartitionSpec]</code> <p>Partitioning specification describing the input sharding. Must be a single <code>PartitionSpec</code> or a single-element container containing one.</p> required <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding when the local shard size is not a multiple of <code>T_A</code>. If False the caller must provide an input whose shape already meets the kernel requirements.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Array</code> <code>jax.Array | tuple[jax.Array, int]</code> <p>The remapped 2D array with the same logical shape and dtype as <code>a</code>. Any temporary padding is removed before returning. The returned array retains the same sharding specification as the input.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>in_specs</code> is not a <code>PartitionSpec</code> or a single-element container containing one.</p> <code>ValueError</code> <p>If <code>in_specs</code> does not indicate row sharding (i.e. <code>P(&lt;axis_name&gt;, None)</code>) or if a container is provided with length != 1.</p> <code>AssertionError</code> <p>If <code>a</code> is not 2D or if <code>pad=False</code> but the provided shape does not satisfy the kernel's non-padded layout requirements.</p> Notes <ul> <li>The FFI call is executed with <code>donate_argnums=0</code> so the input buffer   may be donated to the native kernel for zero-copy performance.</li> <li>Padding calculation is performed with :func:<code>calculate_padding</code> and   the helpers :func:<code>pad_rows</code> / :func:<code>unpad_rows</code> are used when   <code>pad=True</code>.</li> <li>The function assumes the mesh/device count evenly partitions the   first (row) dimension; local shard size is computed as <code>N_rows // ndev</code>.</li> </ul>"},{"location":"api/potri/","title":"jaxmg.potri","text":""},{"location":"api/potri/#jaxmg.potri","title":"<code>jaxmg.potri(a, T_A, mesh, in_specs, return_status=False, pad=True)</code> <code></code>","text":"<p>Compute the inverse of a symmetric matrix using the multi-GPU potri native kernel.</p> <p>Prepares inputs for the native <code>potri_mg</code> kernel and executes it via <code>jax.ffi.ffi_call</code> under <code>jax.jit</code> and <code>jax.shard_map</code>. Handles per-device padding driven by <code>T_A</code> and symmetrizes the result before returning.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>A 2D JAX array of shape <code>(N_rows, N)</code>. Must be symmetric and is expected to be sharded across the mesh along the first (row) axis using <code>P(&lt;axis_name&gt;, None)</code>.</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough. In principle, the larger <code>T_A</code> the faster the solver runs.</p> required <code>mesh</code> <code>Mesh</code> <p>JAX device mesh used for <code>jax.shard_map</code>.</p> required <code>in_specs</code> <code>PartitionSpec or tuple / list[PartitionSpec]</code> <p>PartitionSpec describing the input sharding (row sharding). May be provided as a single <code>PartitionSpec</code> or a single-element container containing one.</p> required <code>return_status</code> <code>bool</code> <p>If True return <code>(A_inv, status)</code> where <code>status</code> is a host-replicated int32 from the native solver. If False return <code>A_inv</code> only. Default is False.</p> <code>False</code> <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to meet <code>T_A</code> requirements; if False the caller must supply already- padded shapes.</p> <code>True</code> <p>Returns:</p> Type Description <code>jax.Array | tuple[jax.Array, int]</code> <p>Array or (Array, int): The inverted matrix (row-sharded). If <code>return_status=True</code> also return the native solver status code.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>in_specs</code> is not a <code>PartitionSpec</code> or a single- element container.</p> <code>ValueError</code> <p>If <code>in_specs</code> does not indicate row sharding (<code>P(&lt;axis_name&gt;, None)</code>).</p> <code>AssertionError</code> <p>If <code>a</code> is not 2D or if required shapes do not match when <code>pad=False</code>.</p> Notes <ul> <li>The FFI call is executed with <code>donate_argnums=0</code> enabling zero-copy   buffer sharing with the native library.</li> <li>If the native solver fails the output may contain NaNs and <code>status</code>   will be non-zero.</li> </ul>"},{"location":"api/potri/#jaxmg.potri_shardmap_ctx","title":"<code>jaxmg.potri_shardmap_ctx(a, T_A, pad=True)</code> <code></code>","text":"<p>Compute the inverse of a symmetric matrix for already-sharded inputs.</p> <p>This helper is a lower-level variant of :func:<code>potri</code> intended for environments where the caller already manages sharding/device placement (for example inside a custom <code>shard_map</code> or other placement context). It performs the same per-device padding logic driven by <code>T_A</code> and calls the native <code>potri_mg</code> FFI target directly via <code>jax.ffi.ffi_call</code>.</p> Warning <p>On exit, we return the upper triangular part of <code>A_inv</code>. To achive the full inverse, call <code>jaxmg.potri_symmetrize</code> outside of the shardmap_context.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>Local, row-sharded slice of the global matrix with shape <code>(shard_size, N)</code> where <code>shard_size</code> is the per-device local row count and <code>N</code> is the global matrix dimension. The matrix should be symmetric.</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough. In principle, the larger <code>T_A</code> the faster the solver runs.</p> required <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to <code>a</code> to satisfy <code>T_A</code>. If False the caller must ensure the provided local shape already meets kernel requirements.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>jax.Array | tuple[jax.Array, int]</code> <p><code>(A_inv, status)</code> where <code>A_inv</code> contains the the upper triangular part of the inverted matrix (in  the same local/sharded layout as the input, with padding removed if applied) and <code>status</code> is the int32 status value returned by the native kernel (shape <code>(1,)</code> device array).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>a</code> is not 2D.</p> <code>ValueError</code> <p>If <code>T_A</code> is too large or shape requirements are violated when <code>pad=False</code>.</p> Notes <ul> <li>This function does not create an outer <code>jax.shard_map</code> or apply   <code>donate_argnums</code>; it is intended for use when the caller already   controls sharding and device placement.</li> <li>Padding is handled with :func:<code>calculate_padding</code>, :func:<code>pad_rows</code>,   and :func:<code>unpad_rows</code>.</li> <li>If the native solver fails the output may contain NaNs and the   returned <code>status</code> will be non-zero.</li> </ul>"},{"location":"api/potrs/","title":"jaxmg.potrs","text":""},{"location":"api/potrs/#jaxmg.potrs","title":"<code>jaxmg.potrs(a, b, T_A, mesh, in_specs, return_status=False, pad=True)</code> <code></code>","text":"<p>Solve the linear system A x = B using the multi-GPU potrs native kernel.</p> <p>Prepares inputs for the native <code>potrs_mg</code> kernel and executes it via <code>jax.ffi.ffi_call</code> under <code>jax.jit</code> and <code>jax.shard_map</code>. Handles per-device padding driven by <code>T_A</code> and returns the solution (and optionally a host-side solver status).</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>2D, symmetric matrix representing the coefficient matrix. Expected to be sharded across the mesh along the first (row) axis using a single <code>PartitionSpec</code>: <code>P(&lt;axis_name&gt;, None)</code>.</p> required <code>b</code> <code>Array</code> <p>2D right-hand side. Expected to be replicated across devices with <code>PartitionSpec</code> <code>P(None, None)</code>.</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough. In principle, the larger <code>T_A</code> the faster the solver runs.</p> required <code>mesh</code> <code>Mesh</code> <p>JAX Mesh object used for <code>jax.shard_map</code>.</p> required <code>in_specs</code> <code>tuple[list][PartitionSpec]</code> <p>The sharding specifications for <code>(a, b)</code>. Expected to be <code>(P(&lt;axis_name&gt;, None), P(None, None))</code>.</p> required <code>return_status</code> <code>bool</code> <p>If True return <code>(x, status)</code> where <code>status</code> is a host-replicated int32 from the native solver. If False return <code>x</code> only. Default is False.</p> <code>False</code> <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to <code>a</code> so each local shard length is compatible with <code>T_A</code>; if False the caller must ensure shapes already match the kernel's requirements.</p> <code>True</code> <p>Returns:</p> Type Description <code>jax.Array | tuple[jax.Array, int]</code> <p>Array or (Array, int): The solution <code>x</code> (replicated across devices). If <code>return_status=True</code> also return the native solver status.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>a</code> or <code>b</code> are not 2D, or their shapes are incompatible.</p> <code>ValueError</code> <p>If <code>in_specs</code> is not a 2-element sequence or if the provided <code>PartitionSpec</code> objects do not match the required patterns (<code>P(&lt;axis_name&gt;, None)</code> for <code>a</code> and <code>P(None, None)</code> for <code>b</code>).</p> Notes <ul> <li>The FFI call may donate the <code>a</code> buffer (<code>donate_argnums=0</code>) for   zero-copy interaction with the native library.</li> <li>If the native solver fails the returned solution may contain NaNs and   <code>status</code> will be non-zero.</li> </ul>"},{"location":"api/potrs/#jaxmg.potrs_shardmap_ctx","title":"<code>jaxmg.potrs_shardmap_ctx(a, b, T_A, pad=True)</code> <code></code>","text":"<p>Solve A x = B by invoking the native multi-GPU potrs kernel without shard_map.</p> <p>This helper is a lightweight, lower-level variant of :func:<code>jaxmg.potrs</code> intended for contexts where the input <code>a</code> is already laid out and sharded at the application level (for example when running inside a custom <code>shard_map</code>/pjit-managed context). It performs the same padding logic driven by <code>T_A</code> and directly calls the native <code>potrs_mg</code> FFI targets  via <code>jax.ffi.ffi_call</code> instead of constructing an additional <code>shard_map</code> wrapper.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>2D coefficient matrix of shape <code>(N_rows // ndev, N)</code>. Must be symmetric for correct solver behavior.</p> required <code>b</code> <code>Array</code> <p>2D right-hand side. Its first dimension must equal the number of columns of <code>a</code> (i.e. <code>a.shape[1] == b.shape[0]</code>).</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough. In principle, the larger <code>T_A</code> the faster the solver runs.</p> required <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to <code>a</code> so each local shard length is compatible with <code>T_A</code>. If False the caller must ensure shapes already meet the kernel's requirements.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[jax.Array, jax.Array]</code> <p><code>(x, status)</code> where <code>x</code> is the solver result (same shape as <code>b</code>) and <code>status</code> is the int32 status value returned by the native kernel (shape <code>(1,)</code> device array).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input arrays are not 2D or their shapes are incompatible.</p> Notes <ul> <li>This function does not perform sharding via <code>jax.shard_map</code> and   therefore must be called only in a shard_map context.</li> <li>Because it does not use <code>donate_argnums</code>, the input buffers are   not donated to the FFI call (no zero-copy donation semantics).</li> </ul>"},{"location":"api/syevd/","title":"jaxmg.syevd","text":""},{"location":"api/syevd/#jaxmg.syevd","title":"<code>jaxmg.syevd(a, T_A, mesh, in_specs, return_eigenvectors=True, return_status=False, pad=True)</code> <code></code>","text":"<p>Compute eigenvalues (and optionally eigenvectors) of a symmetric matrix via the multi-GPU syevd kernel.</p> <p>Prepares the input and executes the appropriate native cuSolverMg kernel (<code>syevd_mg</code> when eigenvectors are requested or <code>syevd_no_V_mg</code> when not) via <code>jax.ffi.ffi_call</code> under <code>jax.jit</code> and <code>jax.shard_map</code>. Handles per-device padding driven by <code>T_A</code> and returns eigenvalues and, optionally, eigenvectors and a host-side status.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>A 2D JAX array of shape <code>(N_rows, N)</code>. Must be symmetric and is expected to be sharded across the mesh along the first (row) axis using <code>P(&lt;axis_name&gt;, None)</code>.</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough.  The Cusolver implementation enforces an upper bound of <code>T_A &lt;= 1024</code>.</p> required <code>mesh</code> <code>Mesh</code> <p>JAX device mesh used for <code>jax.shard_map</code>.</p> required <code>in_specs</code> <code>PartitionSpec or tuple / list[PartitionSpec]</code> <p>PartitionSpec describing the input sharding (row sharding). May be provided as a single <code>PartitionSpec</code> or a single-element container containing one.</p> required <code>return_eigenvectors</code> <code>bool</code> <p>If True (default) compute and return eigenvectors alongside eigenvalues. Eigenvectors are returned row-sharded to the same layout as the input and will be unpadded if padding was applied.</p> <code>True</code> <code>return_status</code> <code>bool</code> <p>If True append a host-replicated int32 solver status to the return values. Default is False.</p> <code>False</code> <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to meet <code>T_A</code> requirements; if False the caller must supply already- correct shapes.</p> <code>True</code> <p>Returns:</p> Type Description <code>jax.Array | tuple[jax.Array, jax.Array] | tuple[jax.Array, jax.Array, int] | tuple[jax.Array, int]</code> <p>Depending on <code>return_eigenvectors</code> and <code>return_status</code>, one of: - eigenvalues (Array of shape <code>(N,)</code>) - (eigenvalues, eigenvectors) - (eigenvalues, status) - (eigenvalues, eigenvectors, status)</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>in_specs</code> is not a <code>PartitionSpec</code> or a single- element container.</p> <code>ValueError</code> <p>If <code>in_specs</code> does not indicate row sharding (<code>P(&lt;axis_name&gt;, None)</code>) or if <code>T_A</code> exceeds implementation limits.</p> <code>AssertionError</code> <p>If <code>a</code> is not 2D or if shape requirements are violated when <code>pad=False</code>.</p> Notes <ul> <li>Eigenvectors (when requested) are returned in the same   row sharding as the input.</li> <li>The FFI call can donate the input buffer (<code>donate_argnums=0</code>) to   enable zero-copy interaction with the native library.</li> <li>If the native solver fails the outputs may contain NaNs and the   status (when requested) will be non-zero.</li> </ul>"},{"location":"api/syevd/#jaxmg.syevd_shardmap_ctx","title":"<code>jaxmg.syevd_shardmap_ctx(a, T_A, return_eigenvectors=True, pad=True)</code> <code></code>","text":"<p>Compute eigenvalues (and optionally eigenvectors) for row-sharded inputs without shard_map wiring.</p> <p>This helper is a lightweight, lower-level variant of :func:<code>syevd</code> intended for contexts where the input <code>a</code> is already laid out and sharded at the application level (for example when running inside a custom <code>shard_map</code>/pjit-managed context). It performs the same padding logic driven by <code>T_A</code> and directly calls the native <code>syevd_mg</code> / <code>syevd_no_V_mg</code> FFI targets via <code>jax.ffi.ffi_call</code> instead of constructing an additional <code>shard_map</code> wrapper.</p> Tip <p>If the shards of the matrix cannot be padded with tiles of size <code>T_A</code> (<code>N / num_gpus % T_A != 0</code>) we have to add padding to fit the last tile. This requires copying the matrix, which we want to avoid at all costs for  large <code>N</code>. Make sure you pick <code>T_A</code> large enough (&gt;=128) and such that it can evenly cover the shards. In principle, increasing <code>T_A</code> will increase  performance at the cost of memory, but depending on <code>N</code>, the performance   will saturate.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Array</code> <p>2D JAX array representing the local, row-sharded slice of the global matrix. Shape should be <code>(shard_size, N)</code> where <code>shard_size</code> is the per-device (local) row count and <code>N</code> is the global matrix dimension.</p> required <code>T_A</code> <code>int</code> <p>Tile width used by the native solver. Each  local shard length must be a multiple of <code>T_A</code>. If the user provides a  <code>T_A</code> that is incompatible with the shard size we pad the matrix accordingly. For small tile sizes (<code>T_A</code>&lt; 128), the solver can  be extremely slow, so ensure that <code>T_A</code> is large enough.  The Cusolver implementation enforces an upper bound of <code>T_A &lt;= 1024</code>.</p> required <code>return_eigenvectors</code> <code>bool</code> <p>If True (default) compute and return eigenvectors in addition to eigenvalues. When True the returned eigenvector array has the same local/sharded shape as the input (and will be unpadded if padding was applied).</p> <code>True</code> <code>pad</code> <code>bool</code> <p>If True (default) apply per-device padding to <code>a</code> to satisfy <code>T_A</code>. If False the caller must ensure the provided local shape already meets kernel requirements.</p> <code>True</code> <p>Returns:</p> Type Description <code>jax.Array | tuple[jax.Array, jax.Array] | tuple[jax.Array, jax.Array, int] | tuple[jax.Array, int]</code> <p>One of the following, depending on <code>return_eigenvectors</code> and whether</p> <code>jax.Array | tuple[jax.Array, jax.Array] | tuple[jax.Array, jax.Array, int] | tuple[jax.Array, int]</code> <p>the caller requests status: - <code>eigenvalues</code> (Array, shape <code>(N,)</code>) - <code>(eigenvalues, eigenvectors)</code> - <code>(eigenvalues, status)</code> - <code>(eigenvalues, eigenvectors, status)</code></p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>a</code> is not a 2D array.</p> <code>ValueError</code> <p>If <code>T_A</code> exceeds implementation limits.</p> Notes <ul> <li>This function does not create a <code>jax.shard_map</code> wrapper and does   not set <code>donate_argnums</code>; it is intended for use when the caller   already controls sharding/device placement.</li> <li>Padding is handled via :func:<code>calculate_padding</code>, :func:<code>pad_rows</code>,   and :func:<code>unpad_rows</code> (the latter two are used as local callables   rather than shard_map-wrapped functions).</li> <li>If the native solver fails the outputs may contain NaNs and the   returned <code>status</code> (when present) will be non-zero.</li> </ul>"},{"location":"technical_details/mpmd/","title":"Multiple Process Multiple Devices (MPMD)","text":"<p>In a multi-process context it is not as straightforward to setup memory sharing between processes, especially when it comes to passing around device pointers which are bound to a specific CUDA context. </p> <p>The solution used here is to make use of the cudaIPC documentation, which allows one to export handles to device memory to different processes. In <code>potrs_mp.cu</code>, we achieve this again through shared memory, although now we share the cudaIPC memory handles:</p> <pre><code>ipcGetHandleAndOffset(array_data_A, \n                      shmAipc[currentDevice], \n                      shmoffsetA[currentDevice]);\n</code></pre> <p>A significant complication is that JAX' memory allocation is managed by XLA, which means that device pointers are actually base pointers together with some offset. cudaIPC only exports the base-pointer, so we have to manually pass around the  offset and extract the true pointer:</p> <pre><code>opened_ptrs_A = ipcGetDevicePointers&lt;data_type&gt;(currentDevice, \n                                                nbGpus,\n                                                shmAipc, \n                                                shmoffsetA);\n</code></pre> <p>We gather all the pointers in process 0 and set up the solver in the same way as before. After completion, it is essential to close the memory handles</p> <pre><code>ipcCloseDevicePointers(currentDevice, \n                       opened_ptrs_A.bases, \n                       nbGpus);\n</code></pre> <p>to avoid memory leaks.</p> <p>Note: If you've made it this far and have experience or thoughts on this, please reach out!</p>"},{"location":"technical_details/spmd/","title":"Single Process Multiple Devices (SPMD)","text":"<p>When <code>potrs.cu</code> is called in a <code>jax.shard_map</code> context through the <code>jax.ffi</code> API with a single process for multiple devices,</p> <pre><code>_out, status = jax.ffi.ffi_call(\n            \"potrs_mg\",\n            out_type,\n            input_layouts=input_layouts,\n            output_layouts=output_layouts,\n        )(_a, _b, T_A=T_A)\n</code></pre> <p>a thread will spawn for each available GPU that executes the code in <code>potrs.cu</code>. Each thread will only have access to its local shard in GPU memory through a device pointer. The <code>cuSolverMgPotrf</code> API must be called in a single thread and requires an array of all device pointers containing the shards on each GPU. </p> <p>This raises the following two issues.</p> <ol> <li>We need to synchronise the threads to set up <code>cuSolverMgPotrf</code> and the data. We then need to execute the solver in thread 0 and have the other threads wait for it to finish. However, JAX has spawned the threads and we do not have any explict control over the thread  syncronization.</li> <li>Since each thread only has access to its local shard, we need to somehow make thread 0 aware of the device pointers across all other threads.</li> </ol> <p>We solver the first issue by initializing a global barrier via <code>std::unique_ptr&lt;std::barrier&lt;&gt;&gt; barrier_ptr</code>. Here <code>std::unique_ptr</code> takes care of deleting the barrier when it goes out of scope (when the FFI call finishes). Then, in <code>potrs.cu</code> we use  <pre><code>static std::once_flag barrier_initialized;\nstd::call_once(barrier_initialized, [&amp;](){ sync_point.initialize(nbGpus); });\n</code></pre> to initialize the barrier across all threads. The <code>std::once_flag</code> ensures that the barrier is initialized exactly once so that all threads see the same barrier.</p> <p>We share device pointers between threads through the creation of shared memory:</p> <pre><code>data_type **shmA = get_shm_device_ptrs&lt;data_type&gt;(currentDevice,\n                                                  sync_point, \n                                                  shminfoA, \n                                                  \"shmA\"); \n</code></pre> <p>In each thread, we then assign the device pointer of the local shard to this shared memory:</p> <pre><code>shmA[currentDevice] = array_data_A;\n</code></pre> <p>which we can safely pass to <code>cuSolverMgPotrf</code>: <pre><code>cusolver_status = cusolverMgPotrs(cusolverH, CUBLAS_FILL_MODE_LOWER, N,NRHS, \n                                  reinterpret_cast&lt;void **&gt;(shmA), \n                                  IA, JA, descrA,\n                                  reinterpret_cast&lt;void **&gt;(shmB), \n                                  IB, JB, descrB,\n                                  compute_type,\n                                  reinterpret_cast&lt;void **&gt;(shmwork), \n                                  *shmlwork,\n                                  &amp;info);\n</code></pre></p>"},{"location":"examples/block_cyclic_data_layout/","title":"The importance of Tile size: Performance of the eigensolver","text":"<p>To use cuSolverMg, matrices must be stored in 1D block-cyclic, column-major form. The reason for this is to ensure that all devices participating in a specific routine can perform computations without being blocked by other parts of the computation (see Dongarra 1996). In <code>jaxmg</code>, we perform this transformation on the C++/CUDA level for you.</p> <p>Choosing the tile size to be large enough is crucial for performance, as we will illustrate here. To get an idea of what the tiling looks like, consider the example where we have a \\(100\\times100\\) matrix distributed with tile size <code>T_A=7</code> over 4 GPUS. We can plot what the tiling will look like.</p> <pre><code>from jaxmg import plot_block_to_cyclic\nN = 100\nT_A = 7\nndev = 4\nplot_block_to_cyclic(N, T_A, ndev)\n</code></pre> <pre><code>(&lt;Figure size 800x600 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Column-block sharded (shard_size=25, required padding=3)'}, xlabel='Columns', ylabel='Rows'&gt;,\n        &lt;Axes: title={'center': '1D block-cyclic (tile size = 7)'}, xlabel='Columns', ylabel='Rows'&gt;],\n       dtype=object))\n</code></pre> <p></p> <p>Notice how we need to add extra padding to ensure that each GPU contains a number of columns that is a multiple of <code>T_A</code> (required by CusolverMg). When matrices get larger, it is important to choose <code>T_A</code> to be large enough, since the solver will slow down otherwise. Here we explore this impact for a setup with 3 GPUs. By increasing the tile size, we see that the performance saturates.</p> <pre><code>import jax\njax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P, NamedSharding\nfrom jaxmg import syevd\n\nimport matplotlib.pyplot as plt\nimport time\n\nprint(f\"Devices: {jax.devices()}\")\n# Assumes we have at least one GPU available\ndevices = jax.devices(\"gpu\")\nN = 3**7\nprint(f\"N={N}\")\ndtype = jnp.float64\n# Create random \nA = jax.random.normal(key=jax.random.key(100), shape=(N,N), dtype=dtype)\nA = A + A.T \nb = jnp.ones((N, 1), dtype=dtype)\nndev = len(devices)\n# Make mesh and place data (rows sharded)\nmesh = jax.make_mesh((ndev,), (\"x\",))\n# Call syevd\ntimes = []\ntile_sizes = [2, 16, 32, 128, 256]\nfor T_A in tile_sizes:\n    Adev = jax.device_put(A, NamedSharding(mesh, P(\"x\", None))).block_until_ready()\n    start = time.time()\n    ev, V, status= syevd(A, T_A=T_A, mesh=mesh, in_specs=(P(\"x\", None),), return_status=True)\n    ev.block_until_ready()\n    V.block_until_ready()\n    end = time.time()\n    times.append(end-start)\n    print(f\"Status {status} - Elapsed time {end - start:1.5f}[s]\")\nplt.plot(tile_sizes, times, marker='.', markersize=10)\nplt.xlabel(\"T_A\")\nplt.ylabel(\"Time [s]\")\nplt.grid()\n</code></pre> <pre><code>Devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2)]\nN=2187\nStatus 0 - Elapsed time 11.42180[s]\nStatus 0 - Elapsed time 0.89933[s]\nStatus 0 - Elapsed time 0.77534[s]\nStatus 0 - Elapsed time 0.72692[s]\nStatus 0 - Elapsed time 0.72381[s]\n</code></pre> <p></p> <p>Notice that there is a massive slowdown for <code>T_A=2</code>. </p> <p>When we choose T_A, we want to make sure that we can equally pad each shard. For example, choosing <code>T_A=3**6</code> for a matrix of size <code>N=3**9</code> will ensure that we can pad it evenly.</p> <pre><code>N = 3**9\nT_A = 3**6\nndev = 3\nplot_block_to_cyclic(N, T_A, ndev)\n</code></pre> <pre><code>(&lt;Figure size 800x600 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Column-block sharded (shard_size=6561, required padding=0)'}, xlabel='Columns', ylabel='Rows'&gt;,\n        &lt;Axes: title={'center': '1D block-cyclic (tile size = 729)'}, xlabel='Columns', ylabel='Rows'&gt;],\n       dtype=object))\n</code></pre> <p></p> <p>This avoids creating an additional copy of the matrix that is properly padded. </p> <pre><code>\n</code></pre>"},{"location":"examples/potrs_example/","title":"Cholesky decomposition with jaxmg.potrs","text":"<p>Here, we give an example of calling <code>jax.potrs</code>, which solves the linear system of equations \\(Ax=b\\) for symmetric, positive-definite \\(A\\) via a Cholesky decomposition.</p> <p>The interface of <code>jaxmg.potrs</code> is simple to use; one needs to supply to underlying mesh of the sharded data and specify the input shardings:</p> <pre><code>import jax\njax.config.update(\"jax_enable_x64\", True)\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as P, NamedSharding\nfrom jaxmg import potrs\nprint(f\"Devices: {jax.devices()}\")\n# Assumes we have at least one GPU available\ndevices = jax.devices(\"gpu\")\nN = 12\nT_A = 3\ndtype = jnp.float64\n# Create diagonal matrix and `b` all equal to one\nA = jnp.diag(jnp.arange(N, dtype=dtype) + 1)\nb = jnp.ones((N, 1), dtype=dtype)\nndev = len(devices)\n# Make mesh and place data (rows sharded)\nmesh = jax.make_mesh((ndev,), (\"x\",))\nA = jax.device_put(A, NamedSharding(mesh, P(\"x\", None)))\nb = jax.device_put(b, NamedSharding(mesh, P(None, None)))\n# Call potrs\nout = potrs(A, b, T_A=T_A, mesh=mesh, in_specs=(P(\"x\", None), P(None, None)))\nprint(out)\nexpected_out = 1.0 / (jnp.arange(N, dtype=dtype) + 1)\nprint(jnp.allclose(out.flatten(), expected_out))\n</code></pre> <pre><code>mkdir -p failed for path /home/rwiersema/.cache/matplotlib: [Errno 13] Permission denied: '/home/rwiersema'\nMatplotlib created a temporary cache directory at /tmp/matplotlib-kkpjcj85 because there was an issue with the default path (/home/rwiersema/.cache/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n</code></pre> <pre><code>Devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2)]\n[[1.        ]\n [0.5       ]\n [0.33333333]\n [0.25      ]\n [0.2       ]\n [0.16666667]\n [0.14285714]\n [0.125     ]\n [0.11111111]\n [0.1       ]\n [0.09090909]\n [0.08333333]]\nTrue\n</code></pre> <p>We retrieve the inverse of the diagonal since we set \\(b\\) equal to a vector of ones. </p> <p>The API <code>jaxmg.potrs</code> is useful if you are not working in a <code>jax.shard_map</code> context, since <code>jaxmg.potrs</code> takes care of the shard map for you. However, in a typical application we may already be working in a sharded context. We therefore provide an additional API that can be called from such a context.</p> <pre><code>from jaxmg import potrs_shardmap_ctx\nfrom functools import partial\n\nprint(f\"Devices: {jax.devices()}\")\n\ndef shard_mapped_fn(_a, _b, _T_A):\n    _a = _a * (jax.lax.axis_index(\"x\")+1) # Multiply each shard with the axis number\n    return potrs_shardmap_ctx(_a, _b, _T_A)\n\ndef my_fn(_a, _b, _T_A):\n    out = jax.shard_map(\n        partial(shard_mapped_fn, _T_A=_T_A),\n        mesh=mesh,\n        in_specs=(P(\"x\", None), P(None, None)),\n        out_specs=(P(None, None), P(None)), # we always return a status.\n        check_vma=False,\n    )(_a, _b)\n    return out\n\n\n# Assumes we have at least one GPU available\ndevices = jax.devices(\"gpu\")\nN = 6\nT_A = 1\ndtype = jnp.float32\n# Create diagonal matrix\nA = jnp.eye(N, dtype=dtype)\nb = jnp.ones((N, 1), dtype=dtype)\nndev = len(devices)\n# Make mesh and place data (rows sharded)\nmesh = jax.make_mesh((ndev,), (\"x\",))\nA = jax.device_put(A, NamedSharding(mesh, P(\"x\", None)))\nb = jax.device_put(b, NamedSharding(mesh, P(None, None)))\n# Call potrs\nout, status = my_fn(A, b, T_A)\nprint(out.block_until_ready())\nprint(f\"Solver status: {status}\")\n</code></pre> <pre><code>Devices: [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2)]\n[[1.        ]\n [1.        ]\n [0.49999997]\n [0.49999997]\n [0.3333333 ]\n [0.3333333 ]]\nSolver status: [0]\n</code></pre> <p>After multiplying the diagonal with the axis number <code>\"x\"</code> we get the the expected solution \\((1,1,\\frac{1}{2},\\frac{1}{2},\\frac{1}{3},\\frac{1}{3})\\)</p>"},{"location":"examples/spmd_mpmd/","title":"MPMD support","text":"<p>We support two modes of distributed computing. Single Process Multiple Devices mode (SPMD), where we have a single process per node that potentially manages multiple devices. We also support MPMD mode (MPMD). Here the user needs to use one process for each GPU. When <code>jaxmg</code> is imported, we attempt to verify the user's distributed setup to not go out beyond these two modes of computation.</p> <p><code>jaxmg</code> supports multi-process <code>jax.distributed</code> environments but cuSolverMg can only run on a single node. There are some technical reasons for this that will hopefully be resolved in a future release.</p> <p>To circumvent this limitation, one can perform a computation over all global devices, replicate the results over all host by gathering the data and calling the solver only on each machine.</p> <p>Here we provide an example of using <code>jaxmg</code> in a context where we have 2 nodes, each with 4 GPUs. In order to use the solver, we will have to gather the results onto each node by making use of a 2D <code>Mesh</code>.</p> <pre><code># Call ./examples/multi_process.sh to launch this code!\nimport os\nimport sys\n\nproc_id = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 0\nnum_procs = int(sys.argv[2]) if len(sys.argv) &gt; 2 else 1\n\n# initialize the distributed system\nimport jax\n\njax.config.update(\"jax_platform_name\", \"cpu\")\njax.distributed.initialize(\"localhost:6000\", num_procs, proc_id)\n\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding\nimport jax.numpy as jnp\nimport numpy as np\n\ndef get_device_grid():\n    by_proc = {}\n    for d in jax.devices():\n        by_proc.setdefault(d.process_index, []).append(d)\n    hosts = sorted(by_proc)\n    return np.array(\n        [[by_proc[h][x] for x in range(jax.local_device_count())] for h in hosts]\n    )\n\ndef create_2d_mesh():\n    dev_grid = get_device_grid()\n    return Mesh(dev_grid, (\"x\", \"y\"))\n\ndef create_1d_mesh():\n    dev_grid = get_device_grid()\n    return Mesh(dev_grid.flatten(), (\"y\",))\n\nprint(f\"Rank {proc_id}\")\nprint(f\"Local devices {jax.local_device_count()}\")\nprint(f\"Global devices {jax.device_count()}\")\nprint(f\"World size {num_procs}\")\nprint(f\"Device grid{get_device_grid()}\")\n</code></pre> <p>When we launch this code like this: <pre><code>#!/bin/bash\n\nexport JAX_NUM_CPU_DEVICES=4\nnum_processes=2\n\nrange=$(seq 0 $(($num_processes - 1)))\n\nfor i in $range; do\n  python multi_process.py $i $num_processes &gt; /tmp/multi_process_$i.out &amp;\ndone\n\nwait\n\nfor i in $range; do\n  echo \"=================== process $i output ===================\"\n  cat /tmp/multi_process_$i.out\n  echo\ndone\n</code></pre> we see <pre><code>=================== process 0 output ===================\nRank 0\nLocal devices 4\nGlobal devices 8\nWorld size 2\nDevice grid\n [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074) CpuDevice(id=131075)]]\n =================== process 1 output ===================\nRank 1\nLocal devices 4\nGlobal devices 8\nWorld size 2\nDevice grid\n [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074) CpuDevice(id=131075)]]\n</code></pre> We can then construct a matrix that has its columns sharded over all global devices, and gather the columns onto each host:</p> <pre><code>mesh2d = create_2d_mesh()\n\nA = jax.device_put(\n    jnp.diag(jnp.arange(1, jax.device_count() + 1, dtype=jnp.float32)),\n    NamedSharding(mesh2d, P(None, (\"x\", \"y\"))),\n)\n\nfor shard in A.addressable_shards:\n    print(f\"shard\\n {shard.data}\")\n\n# Gather over the number of hosts\nA = jax.lax.with_sharding_constraint(A, NamedSharding(mesh2d, P(None, \"y\")))\n\nfor shard in A.addressable_shards:\n    print(f\"shard\\n {shard.data}\")\n</code></pre> <p>which prints <pre><code>=================== process 0 output ===================\nRank 0\nLocal devices 4\nGlobal devices 8\nWorld size 2\nDevice grid\n [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074)\n  CpuDevice(id=131075)]]\nshard\n [[1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]]\n...\nshard\n [[0.]\n [0.]\n [0.]\n [4.]\n [0.]\n [0.]\n [0.]\n [0.]]\nshard\n [[1. 0.]\n [0. 2.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\n...\nshard\n [[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [7. 0.]\n [0. 8.]]\n\n=================== process 1 output ===================\n...\nshard\n [[0.]\n [0.]\n [0.]\n [0.]\n [5.]\n [0.]\n [0.]\n [0.]]\nshard\n ...\nshard\n [[0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [8.]]\nshard\n [[1. 0.]\n [0. 2.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\n...\nshard\n [[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [7. 0.]\n [0. 8.]]\n</code></pre></p> <p>We went from a matrix that was column sharded over all 8 global devices, to a matrix that was column sharded over the 4 gpus in each process.</p> <p>In this host-replicated layout we can safely call <code>jaxmg.potrs</code> on the array with a 2D mesh (the code below only works if we are actually performing this computation with access to GPUs):</p> <pre><code>from jaxmg import potrs\nout= potrs(\n    A,\n    jnp.ones((jax.device_count(), 1), dtype=jnp.float32),\n    T_A=256,\n    mesh=mesh2d,\n    in_specs=(P(None, \"T\"), P(None, None)),\n)\n</code></pre>"}]}