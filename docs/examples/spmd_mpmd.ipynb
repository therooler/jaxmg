{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce0b95f",
   "metadata": {},
   "source": [
    "# SPMD and MPMD support\n",
    "\n",
    "We support two modes of distributed computing. Single Process Multiple Devices mode (SPMD), where we have\n",
    "a single process per node that potentially manages multiple devices. We also support MPMD mode (MPMD). Here the user needs to\n",
    "use one process for each GPU. When `jaxmg` is imported, we attempt to verify the user's distributed setup to not go out beyond these two modes of computation.\n",
    "\n",
    "`jaxmg` supports multi-process `jax.distributed` environments but cuSolverMg **can only run on a single node**. There are some technical reasons for this (see below) that will hopefully be resolved in a future release.\n",
    "\n",
    "To circumvent this limitation, one can perform a computation over all global devices, replicate the results over all host by gathering the data and calling the solver only on the process-local devices. \n",
    "\n",
    "Here we provide an example of using `jaxmg` in a context where we have 2 nodes, each with 4 GPUs.\n",
    "In order to use the solver, we will have to gather the results onto each node by making use of a 2D `Mesh`. To illustrate the data layout, we will simply work with CPUs which allows us to run this code on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call ./examples/multi_process.sh to launch this code!\n",
    "import os\n",
    "import sys\n",
    "\n",
    "proc_id = int(sys.argv[1]) if len(sys.argv) > 1 else 0\n",
    "num_procs = int(sys.argv[2]) if len(sys.argv) > 2 else 1\n",
    "\n",
    "# initialize the distributed system\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "jax.distributed.initialize(\"localhost:6000\", num_procs, proc_id)\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "def get_device_grid():\n",
    "    by_proc = {}\n",
    "    for d in jax.devices():\n",
    "        by_proc.setdefault(d.process_index, []).append(d)\n",
    "    hosts = sorted(by_proc)\n",
    "    return np.array(\n",
    "        [[by_proc[h][x] for x in range(jax.local_device_count())] for h in hosts]\n",
    "    )\n",
    "\n",
    "def create_2d_mesh():\n",
    "    dev_grid = get_device_grid()\n",
    "    return Mesh(dev_grid, (\"x\", \"y\"))\n",
    "\n",
    "def create_1d_mesh():\n",
    "    dev_grid = get_device_grid()\n",
    "    return Mesh(dev_grid.flatten(), (\"y\",))\n",
    "\n",
    "print(f\"Rank {proc_id}\")\n",
    "print(f\"Local devices {jax.local_device_count()}\")\n",
    "print(f\"Global devices {jax.device_count()}\")\n",
    "print(f\"World size {num_procs}\")\n",
    "print(f\"Device grid{get_device_grid()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c905331",
   "metadata": {},
   "source": [
    "When we launch this code like this:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "export JAX_NUM_CPU_DEVICES=4\n",
    "num_processes=2\n",
    "\n",
    "range=$(seq 0 $(($num_processes - 1)))\n",
    "\n",
    "for i in $range; do\n",
    "  python multi_process.py $i $num_processes > /tmp/multi_process_$i.out &\n",
    "done\n",
    "\n",
    "wait\n",
    "\n",
    "for i in $range; do\n",
    "  echo \"=================== process $i output ===================\"\n",
    "  cat /tmp/multi_process_$i.out\n",
    "  echo\n",
    "done\n",
    "```\n",
    "we see\n",
    "```\n",
    "=================== process 0 output ===================\n",
    "Rank 0\n",
    "Local devices 4\n",
    "Global devices 8\n",
    "World size 2\n",
    "Device grid\n",
    " [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
    " [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074) CpuDevice(id=131075)]]\n",
    " =================== process 1 output ===================\n",
    "Rank 1\n",
    "Local devices 4\n",
    "Global devices 8\n",
    "World size 2\n",
    "Device grid\n",
    " [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
    " [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074) CpuDevice(id=131075)]]\n",
    "```\n",
    "We can then construct a matrix that has its columns sharded over all global devices, and gather the columns onto each host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d92396",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh2d = create_2d_mesh()\n",
    "\n",
    "A = jax.device_put(\n",
    "    jnp.diag(jnp.arange(1, jax.device_count() + 1, dtype=jnp.float32)),\n",
    "    NamedSharding(mesh2d, P(None, (\"x\", \"y\"))),\n",
    ")\n",
    "\n",
    "for shard in A.addressable_shards:\n",
    "    print(f\"shard\\n {shard.data}\")\n",
    "\n",
    "# Gather over the number of hosts\n",
    "A = jax.lax.with_sharding_constraint(A, NamedSharding(mesh2d, P(None, \"y\")))\n",
    "\n",
    "for shard in A.addressable_shards:\n",
    "    print(f\"shard\\n {shard.data}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae8c6ae",
   "metadata": {},
   "source": [
    "which prints\n",
    "```\n",
    "=================== process 0 output ===================\n",
    "Rank 0\n",
    "Local devices 4\n",
    "Global devices 8\n",
    "World size 2\n",
    "Device grid\n",
    " [[CpuDevice(id=0) CpuDevice(id=1) CpuDevice(id=2) CpuDevice(id=3)]\n",
    " [CpuDevice(id=131072) CpuDevice(id=131073) CpuDevice(id=131074)\n",
    "  CpuDevice(id=131075)]]\n",
    "shard\n",
    " [[1.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "...\n",
    "shard\n",
    " [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [4.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "shard\n",
    " [[1. 0.]\n",
    " [0. 2.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]]\n",
    "...\n",
    "shard\n",
    " [[0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [7. 0.]\n",
    " [0. 8.]]\n",
    "\n",
    "=================== process 1 output ===================\n",
    "...\n",
    "shard\n",
    " [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [5.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "shard\n",
    " ...\n",
    "shard\n",
    " [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [8.]]\n",
    "shard\n",
    " [[1. 0.]\n",
    " [0. 2.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]]\n",
    "...\n",
    "shard\n",
    " [[0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [7. 0.]\n",
    " [0. 8.]]\n",
    "```\n",
    "\n",
    "We went from a matrix that was column sharded over all 8 global devices, to a matrix that was column sharded over the 4 gpus in each process.\n",
    "\n",
    "In this host-replicated layout we can safely call `jaxmg.potrs` on the array with a 2D mesh (the code below only works if we are actually performing this computation with access to GPUs):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jaxmg import potrs\n",
    "out= potrs(\n",
    "    A,\n",
    "    jnp.ones((jax.device_count(), 1), dtype=jnp.float32),\n",
    "    T_A=256,\n",
    "    mesh=mesh2d,\n",
    "    in_specs=(P(None, \"T\"), P(None, None)),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
