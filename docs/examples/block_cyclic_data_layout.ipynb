{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9cb46e",
   "metadata": {},
   "source": [
    "# Block-cyclic data layout\n",
    "\n",
    "To use cuSolverMg, matrices must be stored in **1D block-cyclic, column-major form**. The reason for this is to ensure that all devices participating in a specific routine can perform computations without being blocked by other parts of the computation (see Dongarra 1996). In `jaxmg`, we handles this transformation on the JAX side with a single **all-to-all** within a `jax.shard_map` context.\n",
    "\n",
    "Consider the case where we have 2 GPUs available and we are trying to solve the linear \n",
    "system $A\\cdot x =b$, where $A$ is an $12\\times12$, positive-definite matrix and $b$ corresponds to a vector of ones. Every shard on each GPU will be of size $12\\times 6$.\n",
    "We require a cyclic 1D tiling with tile size `T_A=2` for `cuSolverMg` to work:\n",
    "\n",
    "<p align=\"center\"><img src=\"../../_static/mat_example.png\" alt=\"Matrix layout illustration\" width=\"500\"></p>\n",
    "\n",
    "\n",
    "In order to interweave the blocks, we need to ensure that each shard is a multiple of\n",
    "`ndev * T_A = 2`, so that we can reshape to `(ndev, T_A, ...)` and exchange the blocks via `jax.lax.all_to_all`. We therefore add zero padding of 2 columns to each shard (see top figure). After interweaving the blocks, we are left with extra padding on the right, which we ignore in the solver itself. After the solver is called, we again use a\n",
    "single `jax.lax.all_to_all` call to remap the data back to block-sharded form. \n",
    "\n",
    "We can achieve this layout in `jaxmg` with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee67b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding\n",
    "from jaxmg import cyclic_1d_layout\n",
    "\n",
    "# Assumes we have at least one GPU available\n",
    "devices = jax.devices(\"gpu\")\n",
    "assert len(devices) in [1, 2], \"Example only works for 1 or 2 devices\"\n",
    "N = 12\n",
    "T_A = 2\n",
    "dtype = jnp.float32\n",
    "ndev = jax.device_count()\n",
    "# Create diagonal matrix and `b` all equal to one\n",
    "A = jnp.diag(jnp.arange(N, dtype=dtype) + 1)\n",
    "mesh = jax.make_mesh((ndev,), (\"x\",))\n",
    "A = jax.device_put(A, NamedSharding(mesh, P(None, \"x\")))\n",
    "A_bc = cyclic_1d_layout(A, T_A)\n",
    "\n",
    "for shard in A_bc.addressable_shards:\n",
    "    print(f\"shard{shard.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3ce30",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "A more involved example is the case where we have 4 GPUS, `N=100` and we want a tiling of `T_A=4`. \n",
    "Now we need a padding of 7 on each GPU in order to perform data remappping (produced with above code):\n",
    "\n",
    "<p align=\"center\"><img src=\"../../_static/mat.png\" alt=\"Matrix layout illustration\" width=\"500\"></p>\n",
    "\n",
    "- Dongarra, J.J., and D.W. Walker. *The Design of Linear Algebra Libraries for High Performance Computers.* Office of Scientific and Technical Information (OSTI), August 1, 1993. https://doi.org/10.2172/10184308."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
